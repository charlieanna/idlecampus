#!/bin/bash

# UPSC Platform - Ollama AI Setup Script
# This script sets up Ollama for AI-powered answer evaluation

set -e

echo "ğŸ¤– UPSC Platform - Ollama AI Setup"
echo "===================================="
echo ""

# Colors
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
NC='\033[0m' # No Color

# Check if Docker is installed
if ! command -v docker &> /dev/null; then
    echo -e "${RED}âŒ Docker is not installed${NC}"
    echo "Please install Docker first: https://docs.docker.com/get-docker/"
    exit 1
fi

echo -e "${GREEN}âœ… Docker found${NC}"
echo ""

# Check if docker-compose is available
if command -v docker-compose &> /dev/null; then
    DOCKER_COMPOSE="docker-compose"
elif docker compose version &> /dev/null; then
    DOCKER_COMPOSE="docker compose"
else
    echo -e "${RED}âŒ docker-compose is not available${NC}"
    echo "Please install docker-compose"
    exit 1
fi

echo -e "${GREEN}âœ… docker-compose found${NC}"
echo ""

# Start Ollama container
echo "ğŸ“¦ Starting Ollama container..."
$DOCKER_COMPOSE -f docker-compose.ollama.yml up -d

# Wait for Ollama to be ready
echo ""
echo "â³ Waiting for Ollama to start (30 seconds)..."
sleep 30

# Check if Ollama is responding
echo ""
echo "ğŸ” Checking Ollama status..."
if curl -s http://localhost:11434/api/tags > /dev/null; then
    echo -e "${GREEN}âœ… Ollama is running!${NC}"
else
    echo -e "${RED}âŒ Ollama is not responding${NC}"
    echo "Try: docker logs upsc_ollama"
    exit 1
fi

echo ""
echo "ğŸ“¥ Pulling AI model (llama3.2)..."
echo "This may take a few minutes depending on your internet connection..."
docker exec upsc_ollama ollama pull llama3.2

# Verify model is installed
echo ""
echo "ğŸ” Verifying model installation..."
if docker exec upsc_ollama ollama list | grep -q "llama3.2"; then
    echo -e "${GREEN}âœ… Model llama3.2 installed successfully!${NC}"
else
    echo -e "${YELLOW}âš ï¸  Model installation may have failed${NC}"
    echo "Check with: docker exec upsc_ollama ollama list"
fi

# Test the API
echo ""
echo "ğŸ§ª Testing AI service..."
if curl -s http://localhost:11434/api/tags | grep -q "llama3.2"; then
    echo -e "${GREEN}âœ… AI service is working!${NC}"
else
    echo -e "${YELLOW}âš ï¸  Could not verify AI service${NC}"
fi

# Setup environment variables
echo ""
echo "ğŸ“ Setting up environment variables..."
if [ ! -f backend/.env ]; then
    touch backend/.env
fi

# Add Ollama config if not present
if ! grep -q "OLLAMA_URL" backend/.env; then
    echo "" >> backend/.env
    echo "# Ollama AI Configuration" >> backend/.env
    echo "OLLAMA_URL=http://localhost:11434" >> backend/.env
    echo "OLLAMA_MODEL=llama3.2" >> backend/.env
    echo -e "${GREEN}âœ… Environment variables added to backend/.env${NC}"
else
    echo -e "${YELLOW}âš ï¸  Ollama config already exists in backend/.env${NC}"
fi

echo ""
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo -e "${GREEN}ğŸ‰ Ollama AI Setup Complete!${NC}"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo ""
echo "ğŸ“Š Status:"
echo "  â€¢ Ollama running on: http://localhost:11434"
echo "  â€¢ Model installed: llama3.2"
echo "  â€¢ Container name: upsc_ollama"
echo ""
echo "ğŸš€ Next Steps:"
echo "  1. Start Rails: cd backend && rails server"
echo "  2. Test status: curl http://localhost:3000/api/v1/upsc/ai/status"
echo "  3. Submit answers with auto_evaluate=true"
echo ""
echo "ğŸ“š Documentation:"
echo "  â€¢ Setup Guide: OLLAMA_SETUP_GUIDE.md"
echo "  â€¢ API Docs: OPTION_B_DELIVERY_COMPLETE.md"
echo ""
echo "ğŸ”§ Useful Commands:"
echo "  â€¢ View logs: docker logs upsc_ollama"
echo "  â€¢ Stop Ollama: $DOCKER_COMPOSE -f docker-compose.ollama.yml down"
echo "  â€¢ Restart: $DOCKER_COMPOSE -f docker-compose.ollama.yml restart"
echo "  â€¢ Pull different model: docker exec upsc_ollama ollama pull mistral"
echo ""
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
